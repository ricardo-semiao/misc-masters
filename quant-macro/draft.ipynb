{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Ricardo SemiÃ£o e Castro\"\n",
    "warning: false\n",
    "message: false\n",
    "format:\n",
    "  pdf:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    number-sections: false\n",
    "    include-in-header:\n",
    "      text: |\n",
    "        \\usepackage{sectsty}\n",
    "        \\usepackage{etoolbox}\n",
    "        \\patchcmd{\\tableofcontents}{\\thispagestyle{plain}}{\\thispagestyle{plain}\\clearpage}{}{}\n",
    "        \\subsectionfont{\\clearpage}\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files from this problem set can be found in [github.com/ricardo-semiao/task-masters -> quant-macro](https://github.com/ricardo-semiao/task-masters/tree/main/quant-macro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import polars as pl\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set, we will deal with the Huggett (1993) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a) Optimal Decision Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, lets define the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 #number of states\n",
    "states = [1, 0.1] #value of states (unemployed and employed)\n",
    "probs = np.array([[0.925, 0.075], [0.5, 0.5]]) #transition matrix\n",
    "avg_end = probs[1, 1] * states[1] + probs[2, 2] * states[2]\n",
    "\n",
    "beta = 0.96 ** (1 / 6) #discount factor\n",
    "sigma = 1.5 #CRRA utility function parameter\n",
    "r = (1 + 0.34) ** (1 / 6) - 1 #interest rate\n",
    "\n",
    "m = 200 #number of grid points\n",
    "tol = 1e-6 #tolerance level\n",
    "max_iter = 1000 #maximum number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define the grids and other utility objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(-avg_end, 3 * avg_end, m).reshape(m, 1) #equally spaced row-vector for a\n",
    "\n",
    "q_bounds = [(beta + tol), 2] #lowest and highest possible q\n",
    "q = (np.sum(q_bounds)) / 2 #initial guess for q\n",
    "\n",
    "om = np.ones((1, m)) #auxiliary ones vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions For Each Step\n",
    "\n",
    "Now, lets define functions for each step of our algorithm:\n",
    "\n",
    "- Creating the grid points of consumption and utility.\n",
    "- Find the value function trough iteration.\n",
    "- Find the optimal policy functions.\n",
    "- Create the markov chains for the endogenous transition.\n",
    "- Update the distribution accordingly.\n",
    "- Get the final result of the security markets, and adjust `q` for the next iteration.\n",
    "\n",
    "Lets define functions for each step.\n",
    "\n",
    "Starting with functions to create consumption grid points, and the related utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c(s, a, q, om, tol):\n",
    "    c = np.ones((m, 1)) @ np.transpose(a + s) - (q * a) @ om\n",
    "    c[c < 0] = tol\n",
    "    return c\n",
    "\n",
    "def get_u(c, sigma):\n",
    "    return (c**(1 - sigma) - 1) / (1 - sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(m, us, beta, om, tol, probs):\n",
    "    check_vf = np.array([999, 999])\n",
    "    vs = [np.zeros((m, 1)) for i in range(2)]\n",
    "    tvs_partial = [0, 0]\n",
    "    tvs = [np.transpose(np.max(us[i] + beta * vs[i] @ om, axis=0, keepdims=True))\n",
    "        for i in range(2)]\n",
    "    \n",
    "    while np.max(check_vf) > tol:\n",
    "        vs = tvs\n",
    "        for i in range(2):\n",
    "            tvs_partial[i] = us[i] + beta * (probs[i, 0] * vs[0] + probs[i, 1] * vs[1]) * om\n",
    "            tvs[i] = np.max(tvs_partial[i], axis=0, keepdims=True)\n",
    "            check_vf[i] = np.linalg.norm(tvs[i] - vs[i]) / np.linalg.norm(vs[i])\n",
    "\n",
    "    return tvs_partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(tvs_partial, a, states, q):\n",
    "    policy_a = [0, 0]\n",
    "    policy_c = [0, 0]\n",
    "    max_vf_ind = [0, 0]\n",
    "\n",
    "    for i in range(2):\n",
    "        max_vf_ind[i] = np.argmax(tvs_partial[i], axis=0)\n",
    "        policy_a[i] = a[max_vf_ind[i], :]\n",
    "        policy_c[i] = a + states[i] - q * policy_a[i]\n",
    "\n",
    "    return max_vf_ind, policy_a, policy_c    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov(n, m, probs, max_vf_ind):\n",
    "    mkprobs_aux = [None, None]\n",
    "    mkprobs = np.zeros((n * m, n * m))\n",
    "    \n",
    "    for i in range(2):\n",
    "        mkprobs_aux[i] = np.zeros((m, m))\n",
    "        mkprobs_aux[i][:, max_vf_ind[i]] = 1\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            mkprobs[i*2, 2*j] = probs[0, 0] * mkprobs_aux[0][i, j]\n",
    "            mkprobs[i*2, 2*j+1] = probs[0, 1] * mkprobs_aux[0][i, j]\n",
    "            mkprobs[i*2+1, 2*j] = probs[1, 0] * mkprobs_aux[1][i, j]\n",
    "            mkprobs[i*2+1, 2*j+1] = probs[1, 1] * mkprobs_aux[1][i, j]\n",
    "\n",
    "    return mkprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the update of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(n, m, mkprobs, tol):\n",
    "    dist_prev = np.ones((1, n * m)) / (n * m)\n",
    "    dist = dist_prev @ mkprobs\n",
    "    iter_dist = 1\n",
    "    \n",
    "    while np.linalg.norm(dist - dist_prev) < tol:\n",
    "        iter_dist += 1\n",
    "        if iter_dist == 1000:\n",
    "            sprintf('!!! Hit maximum amount of iterations on distribution step',)\n",
    "            break\n",
    "        dist_prev = dist\n",
    "        dist = np.matmul(dist, mkprobs)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Now, we can join it all in the main algorithm loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 || sec = 999, q = 1.4805\n",
      "Iteration: 2 || sec = [[-142.16633267]], q = 1.74025\n",
      "Iteration: 3 || sec = [[-106.]], q = 1.870125\n",
      "Iteration: 4 || sec = [[-89.]], q = 1.9350625\n",
      "Iteration: 5 || sec = [[-81.]], q = 1.96753125\n",
      "Iteration: 6 || sec = [[-77.]], q = 1.983765625\n",
      "Iteration: 7 || sec = [[-75.]], q = 1.9918828125\n",
      "Iteration: 8 || sec = [[-74.]], q = 1.99594140625\n",
      "Iteration: 9 || sec = [[-74.]], q = 1.997970703125\n",
      "Iteration: 10 || sec = [[-73.]], q = 1.9989853515625\n",
      "Iteration: 11 || sec = [[-73.]], q = 1.9994926757812501\n",
      "Iteration: 12 || sec = [[-73.]], q = 1.999746337890625\n",
      "Iteration: 13 || sec = [[-73.]], q = 1.9998731689453124\n",
      "Iteration: 14 || sec = [[-73.]], q = 1.9999365844726562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n",
      "\u001b[0;32m     11\u001b[0m tvs_partial \u001b[38;5;241m=\u001b[39m value_function(m, us, beta, om, tol, probs)\n",
      "\u001b[0;32m     13\u001b[0m max_vf_ind, policy_a, policy_c \u001b[38;5;241m=\u001b[39m policy(tvs_partial, a, states, q)\n",
      "\u001b[1;32m---> 15\u001b[0m mkprobs \u001b[38;5;241m=\u001b[39m \u001b[43mmarkov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_vf_ind\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     17\u001b[0m dist \u001b[38;5;241m=\u001b[39m distribution(n, m, mkprobs, tol)\n",
      "\u001b[0;32m     19\u001b[0m anext \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n \u001b[38;5;241m*\u001b[39m m, \u001b[38;5;241m1\u001b[39m))\n",
      "\n",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mmarkov\u001b[1;34m(n, m, probs, max_vf_ind)\u001b[0m\n",
      "\u001b[0;32m     12\u001b[0m         mkprobs[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m probs[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m mkprobs_aux[\u001b[38;5;241m0\u001b[39m][i, j]\n",
      "\u001b[0;32m     13\u001b[0m         mkprobs[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mj] \u001b[38;5;241m=\u001b[39m probs[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m mkprobs_aux[\u001b[38;5;241m1\u001b[39m][i, j]\n",
      "\u001b[1;32m---> 14\u001b[0m         mkprobs[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m probs[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m mkprobs_aux[\u001b[38;5;241m1\u001b[39m][i, j]\n",
      "\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mkprobs\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "security = 999\n",
    "iter_total = 0\n",
    "\n",
    "while abs(security) > tol:\n",
    "    iter_total += 1\n",
    "    print(f\"Iteration: {iter_total} || sec = {security}, q = {q}\")\n",
    "    \n",
    "    cs = [get_c(s, a, q, om, tol) for s in states]\n",
    "    us = [get_u(c, sigma) for c in cs]\n",
    "\n",
    "    tvs_partial = value_function(m, us, beta, om, tol, probs)\n",
    "\n",
    "    max_vf_ind, policy_a, policy_c = policy(tvs_partial, a, states, q)\n",
    "\n",
    "    mkprobs = markov(n, m, probs, max_vf_ind)\n",
    "\n",
    "    dist = distribution(n, m, mkprobs, tol)\n",
    "\n",
    "    anext = np.zeros((n * m, 1))\n",
    "    for i in range(m):\n",
    "        anext[i * 2, 0] = policy_a[0][i][0]\n",
    "        anext[i * 2 + 1, 0] = policy_a[1][i][0]\n",
    "    \n",
    "    security = dist @ anext\n",
    "    \n",
    "    q_bounds[int(security[0][0] > 0)] = q\n",
    "    q = np.sum(q_bounds) / 2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
